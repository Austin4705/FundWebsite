---
layout: post
title: CS4789
last_updated: 2025-04-27
---

[comment]: THIS FILE IS AUTOMATICALLY GENERATED FROM CS4789Pset4.tex. IT WILL BE REWRITTEN ON CHANGE OF THE .tex FILE!

> **Problem:**
>
> > **Subproblem:**
> >
> > Since we have fixed $A=A^{\pi_{\theta_t}}(s,a)$, we know that
> > $$r(\theta)=\frac{\pi_\theta(a|s)}{\pi_{\theta_t}(a|s)}$$ Thus each
> > term is $$r(\theta)A, (1-\epsilon)A, (1+\epsilon)A$$ If the first
> > term is the min, then the gradient becomes $$\begin{aligned}
> >         \nabla_\theta[r(\theta)]A&=A\nabla_\theta[\frac{\pi_\theta(a|s)}{\pi_{\theta_t}(a|s)}] \\
> >                                  &= A\,\nabla_\theta r(\theta) \\
> >                                  &= A\,\nabla_\theta\!\Bigl[\frac{\pi_\theta(a\mid s)}{\pi_{\theta_t}(a\mid s)}\Bigr] \\
> >                                  &= A\,\frac{\pi_\theta(a\mid s)}{\pi_{\theta_t}(a\mid s)}\,\nabla_\theta\log\pi_\theta(a\mid s) \\
> >                                  &= A\,r(\theta)\,\nabla_\theta\log\pi_\theta(a\mid s).
> >       
> > \end{aligned}$$ For the second term, it becomes $$\begin{aligned}
> >         \nabla_\theta[(1-\epsilon)A] &= 0
> >       
> > \end{aligned}$$ And for the third term, it becomes $$\begin{aligned}
> >         \nabla_\theta[(1+\epsilon)A] &= 0
> >       
> > \end{aligned}$$ Since they are constant with respect to theta.
>
> > **Subproblem:**
> >
> > When $r(\theta)$ drifts above $1+\epsilon$ with $A>0$ or below
> > $1-\epsilon$ with $A<0$, we choose the clipped bound instead of the
> > main function. This causes the gradient to get set to 0, This
> > implies that $r(\theta)$ can't drift above or below these bounds.
> > This means that the polciy ratio cannot increase more than its
> > bounded $[1-\epsilon, 1+\epsilon]$ keeping policy updates close to
> > $\pi_{\theta_t}$.
>
> > **Subproblem:**
> >
> > Suppose instead we use
> > $$\hat l_{\text{final}}(\theta)=\sum_{s,a}\text{clip}\lvert ab(\frac{\pi_\theta(a|s)}{\pi_{\theta_t}}(a|s), 1-\epsilon, 1+\epsilon \rvert\cdot A^{\pi_{\theta_t}}(s,a)$$
> > Instead of having a minimization.
> >
> > Then if $A>0$ and $r<1-\epsilon$ the function without the min would
> > always be clipped by $(1-\epsilon)A$ for any state action pair which
> > has gradient $0$. This implies that if $r$ falls below the
> > $1-\epsilon$ lower bound, there is no ability for the gradient to be
> > in a direction to increase it.
> >
> > This contradicts the function with the min because if it falls below
> > the $1-\epsilon$ bound the gradient pushes $r(\theta)$ up so
> > $min(rA, (1-\epsilon)A)=rA$. This then creates the gradient
> > $A\,r(\theta)\,\nabla_\theta\log\pi_\theta(a\mid s)$ from the
> > previous part, allowing a corrective gradient to bring it back up.
> >
> > Similarly, if $A<0$ and $r>1+\epsilon$ the function without the min
> > would always be clipped by $(1+\epsilon)A$ for any state action pair
> > with gradient $0$. This implies that if $r$ falls above the
> > $1+\epsilon$ upper bound, there is no ability for the gradient to be
> > in a direction to decrease it.
> >
> > This similarly contradicts the function with the min because if it
> > rises above the $1+\epsilon$ bound the gradient pushes $r(\theta)$
> > down so $min(rA, (1+\epsilon)A)=rA$. This then creates the gradient
> > $A\,r(\theta)\,\nabla_\theta\log\pi_\theta(a\mid s)$ from the
> > previous part, allowing a corrective gradient to bring it back down.

> **Problem:**
>
> > **Subproblem:**
> >
> > Set
> > $\pi_\text{ref}=\begin{bmatrix}\frac 13&\frac 13&\frac 13\end{bmatrix}$.
> > Then $\frac13\leq \frac13\leq\frac13$. Now define
> > $\pi_0 = \begin{bmatrix}0.3&0.2&0.5\end{bmatrix}$. Then
> > $0.2\leq0.3\leq0.5$. Thus we get that $$\begin{aligned}
> >         \mathcal{L}_{DPO}(\pi_\theta;\pi_{\text{ref}}) &= -\mathrm{\mathbb{E}}_{x,y_w,y_l\in \mathcal{D}}\lvert \log\sigma(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)}-\beta\log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}) \rvert \\
> >         \intertext{Simplifies to }
> >                                                 &= -\mathrm{\mathbb{E}}_{x,y_w,y_l\in \mathcal{D}}\lvert \log\sigma(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_\theta(y_l|x)}) \rvert \\
> >                                                 \intertext{Since $\pi_\text{ref}(y|x)$ is always equal to $\frac13$. Thus if we sample with single prefrence ordering $y_1\leq y_2$ we get when going from $\pi_0$ to the next iteration}
> >                                                 &= -\mathrm{\mathbb{E}}_{x,y_2,y_1\in \mathcal{D}}\lvert \log\sigma(\beta\log\frac{\pi_\theta(y_2|x)}{\pi_\theta(y_1|x)}) \rvert \\
> >                                                 &= -\mathrm{\mathbb{E}}_{x,y_2,y_1\in \mathcal{D}}\lvert \log\sigma(\beta\log\frac{3}{2}) \rvert \\
> >       
> > \end{aligned}$$ Thus if we create polices $\pi_1, \pi_2$ that have a
> > good loss function, we must maximize the ratio of
> > $\frac{\pi_\theta(y_2|x)}{\pi_\theta(y_1|x)}$. This also implies
> > that polices with equivalent ratios have the same loss function
> > since we are only sampling once. Thus after update the some possible
> > answers are arbitrarly setting $\pi_1(y_3)=\frac12$. Then say
> > $$\frac{\pi_1(y_2)}{\pi_1(y_2)}=1.75=r$$ Thus since probability
> > spaces must stay normalized we are constrained to
> > $$\pi_1(y_1)+1.75\pi_1(y_1)=\frac12\Rightarrow 2.75\pi_1(y_1)=\frac12\Rightarrow\pi_1(y_1)=\frac2{11}\approx0.18, \pi_1(y_2)=\frac{22}{69}\approx0.31$$
> > We could also set $\pi_2(y_3)=\frac14$. Then since the ratio must be
> > 1.5 implies that
> > $$\pi_2(y_1)+1.75\pi_2(y_1)=\frac34\Rightarrow  2.75\pi_2(y_1)=\frac34\Rightarrow\pi_2(y_1)=\frac{3}{11}\approx0.27, \pi_2(y_2)=\frac{21}{44}\approx0.47$$
> > Thus
> > $\pi_1=\begin{bmatrix}\frac 2{11}&\frac{22}{69}&0.5\end{bmatrix}, \pi_2=\begin{bmatrix}\frac{3}{11}&\frac{21}{44}&0.25\end{bmatrix}$
> > wich equivlanet loss functions, but $\pi_1$ satisifes
> > $y_1\leq y_2\leq y_3$ with $\pi_2$ does not. Thus our loss functions
> > become $$\mathcal{L}_{DPO}=\log(\frac1{1+e^{-\beta r}})$$ With $r$
> > being the ratio between elements. Thus the loss with $\beta=1$ is
> > $$\begin{aligned}
> >         \mathcal{L}(\pi)_{\text{ref}}&=\log(\frac1{1+e^{-\beta 1}})\approx -0.31\\
> >         \mathcal{L}(\pi_0)&=\log(\frac1{1+e^{-\beta 1.5}})\approx -0.2\\
> >         \mathcal{L}(\pi_1)&=\log(\frac1{1+e^{-\beta 1.75}})\approx -0.16\\
> >         \mathcal{L}(\pi_2)&=\log(\frac1{1+e^{-\beta 1.75}})\approx-0.16
> >       
> > \end{aligned}$$ This means that DPO has a blindness between
> > unobserved comparisons regarding polices.
>
> > **Subproblem:**
> >
> > For $\pi_{\text{ref}}(\cdot|x), \pi_{\theta}(\cdot|x)$ let
> > $$\begin{array}{|c|c|}
> >         \hline
> >         \pi_{\text{ref}}(y_1|x) & \frac12\\
> >         \pi_{\text{ref}}(y_2|x) & \frac12 \\
> >         \pi_{\text{ref}}(y_3|x) & 0\\
> >         \pi_{\theta}(y_1|x) & 0\\
> >         \pi_{\theta}(y_2|x) & \frac12\\
> >         \pi_{\theta}(y_3|x) & \frac12\\
> >         \hline
> >       \end{array}$$ With
> > $\pi_{\text{ref}}(y_1|x)=\frac12\leq\pi_{\text{ref}}(y_2|x)=\frac12$
> > and $\pi_{\theta}(y_1|x) = 0\leq \pi_{\theta}(y_2|x) = \frac12$ Then
> > we find that $$\begin{aligned}
> >         \mathcal{L}_{DPO}(\pi_\theta;\pi_{\text{ref}}) &= -\mathrm{\mathbb{E}}_{x,y_w,y_l\in \mathcal{D}}\lvert \log\sigma(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)}-\beta\log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}) \rvert \\
> >                                 \intertext{Now since we are only sampling $(y_w,y_l)=(y_2,y_1)$}
> >                                 &= -\mathrm{\mathbb{E}}_{x,y_w,y_l\in \mathcal{D}}\lvert \log\sigma(\beta\log\frac{\pi_\theta(y_w|x)\pi_{\text{ref}}(y_l|x)}{\pi_{\text{ref}}(y_w|x)\pi_\theta(y_l|x)}) \rvert \\
> >                                 \intertext{With notably $\sigma(z)=\frac1{1+e^{-z}}$}
> >                                 &= -\mathrm{\mathbb{E}}_{x,y_2,y_1\in \mathcal{D}}\lvert \log\sigma(\beta\log\frac{\pi_\theta(y_2|x)\pi_{\text{ref}}(y_1|x)}{\pi_{\text{ref}}(y_2|x)\pi_\theta(y_1|x)}) \rvert \\
> >                                 \intertext{Thus plugging in the values we get}
> >                                 &= -\mathrm{\mathbb{E}}_{x,y_2,y_1\in \mathcal{D}}\lvert \log\sigma(\beta\log\frac{\pi_\theta(y_2|x)}{\pi_\theta(y_1|x)}) \rvert \\
> >                                 &= -\mathrm{\mathbb{E}}_{x,y_2,y_1\in \mathcal{D}}\lvert \log\sigma(\beta\log\lvert ab(\frac{\frac12}{0} \rvert) \rvert \\
> >                                 \intertext{With asymotic behavior $\beta\log(\frac{\frac12}{0})=+\infty$ thus}
> >                                 &= -\mathrm{\mathbb{E}}_{x,y_2,y_1\in \mathcal{D}}\lvert \log\frac1{1+e^{-(+\infty)}} \rvert \\
> >                                 &= -\mathrm{\mathbb{E}}_{x,y_2,y_1\in \mathcal{D}}\lvert \log(1) \rvert \\
> >                                 &= 0
> >       
> > \end{aligned}$$ And for KL Difergence we get $$\begin{aligned}
> >         D_{KL}(\pi_\theta||\pi_{\text{ref}}) &= \sum_{i=1}^3\theta(y_i|x)\log\frac{\pi_\theta(y_i|x)}{\pi_{\text{ref}}(y_i|x)} \\
> >                                       &= 0 + \frac12\log\frac{\frac12}{\frac12}+\frac12\log\frac{\frac12}{0}\\
> >                                       &= +\infty
> >       
> > \end{aligned}$$ Thus we shown that we can pick a policy for
> > $\mathcal{L}_{DPO}=0$ and $KL(\pi||\pi_\text{ref})=+\infty$
