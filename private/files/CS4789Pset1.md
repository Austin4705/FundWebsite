---
layout: post
title: Pset 1
last_updated: 2025-05-28
---

[comment]: THIS FILE IS AUTOMATICALLY GENERATED FROM CS4789Pset1.tex. IT WILL BE REWRITTEN ON CHANGE OF THE .tex FILE!

> **Problem:**
>
> > **Subproblem:**
> >
> > We know that for the transition function
> >
> > ::: center
> >    s   a       A           B           C
> >   --- --- ----------- ----------- -----------
> >    A   N   $\frac12$   $\frac12$       0
> >    A   S       0           1           0
> >    B   N   $\frac12$   $\frac12$       0
> >    B   S       0       $\frac12$   $\frac12$
> >    C   N       0           1           0
> >    C   S       0           0           1
> > :::
> >
> > And for the reward function
> >
> > ::: center
> >    **State $s$**   **Action $a$**   **Reward $r(s, a)$**
> >   --------------- ---------------- ----------------------
> >          A               N                   1
> >          A               S                   1
> >          B               N                   -1
> >          B               S                   -1
> >          C               N                   1
> >          C               S                   1
> > :::
>
> > **Subproblem:**
> >
> > Dynamic programming implies that
> > $$Q_h^*(s,a) = r(s,a) + \mathrm{\mathbb{E}}_{s'\sim P(\cdot|s,a)}\lvert V_{h+1}^*(s') \rvert,\, \pi_h^*(s) = \mathrm{argmax}_a Q_h^*(s,a).$$
> > Thus since
> > $Q^*_{H-1}(s,a)=r(s,a),\, \pi_{H-1}^*(s)=\mathrm{argmax}_a Q^*_{H-1}(s,a)$
> > and $V_{H-1}^*(s)=\max_a Q_{H-1}^*(s,a)$.
> >
> > Now since $V_{H-1}^*(A,B,C) = \{1,1-1\}$,
> > $\pi_{H-1}^*(A,B,C) = \{\frac NS,\frac NS, \frac NS\}$ thus using
> > the chart above
> > $$Q_{H-2}^*(A,N) = 1 +\frac12-\frac12 = 1,\, Q_{H_2}^*(A,S) = 1-1=0$$
> > so $\pi_{H-2}^*(A)=N$. And
> > $$Q_{H-2}^*(B,N)= -1,\, Q_{H-2}^*(B,S) = -1$$ so
> > $\pi_{H-2}^*(B) = \frac NS$
> >
> > And $$Q_{H-2}^*(C,N) = 0,\, Q_{H-2}^*(C,S)= 2$$ so
> > $\pi_{H-2}^*(C)=S$
> >
> > Thus since $V_{H-2}^*(A,B,C)= \{1,-1,2\}$ implies that
> > $$Q_{H-3}^*(A,N) =-1,\, Q_{H_3}^*(A,S) = 1-1=0$$ so
> > $\pi_{H-3}^*(A)=N$. And
> > $$Q_{H-3}^*(B,N)= -1,\, Q_{H-3}^*(B,S) = -\frac12$$ so
> > $\pi_{H-3}^*(B) = S$
> >
> > And $$Q_{H-3}^*(C,N) = 0,\, Q_{H-3}^*(C,S)= 3$$ so
> > $\pi_{H-3}^*(C)=S$ This implies that $\pi^*_{H-3} =\{N,S,S\}$ is the
> > optimal policy for our problem as for each timestep it is the
> > optimal policy. Thus $$\pi^*(A,B,C)=\{N,S,S\}$$
>
> > **Subproblem:**
> >
> > We know that
> > $$V^\pi(A)=r(A,N)+\gamma\mathrm{\mathbb{E}}_{s'\sim P(\cdot |A,N)}\lvert V^\pi(s') \rvert = 1+\gamma(\frac12V^\pi(A)+\frac12V^\pi(B))$$
> > and that
> > $$V^\pi(B)=r(B,S)+\gamma\mathrm{\mathbb{E}}_{s'\sim P(\cdot |B,S)}\lvert V^\pi(s') \rvert =-1+\gamma(\frac12V^\pi(B)+\frac12V^\pi(C))$$
> > and that
> > $$V^\pi(C)=r(C,S)+\gamma\mathrm{\mathbb{E}}_{s'\sim P(\cdot |C,S)}\lvert V^\pi(s') \rvert =-1+\gamma V^\pi(C)$$
> > So algebraicly solving for $V^\pi(C)$ we get
> > $$V^\pi(C)==-1+\gamma V^\pi(C)\imples V_\pi(C)=\frac1{1-\gamma}$$
> > And plugging this value into our expression for $V^\pi(B)$ implies
> > that
> > $$V^\pi(B)=-1+\gamma(\frac12V^\pi(B)+\frac12\frac1{1-\gamma})\imples V^\pi(B) = \frac{3\gamma-2}{(1-\gamma)(2-\gamma)}$$
> > And finally using this value into our expression for $V^\pi(A)$
> > implies that
> > $$V^\pi(A)= 1+\gamma(\frac12V^\pi(A)+\frac12\frac{3\gamma-2}{(1-\gamma)(2-\gamma)})\imples V^\pi(A) = \frac{\gamma(3\gamma-2) + 2(1-\gamma)(2-\gamma)}{(1-\gamma)(2-\gamma)^2}$$
>
> > **Subproblem:**
> >
> > Recall that
> >
> > > **Definition:**
> > >
> > > $Q(s,a) = r(s,a) + \gamma \mathbb{E}_{s'~P(.|s,a)}\lvert \max_{a'\in \mathcal{A}}Q(s',a') \rvert$
> >
> > Now since $\pi^*(s)=\mathrm{argmax}_a Q^*(s,a)$ implies that thus
> > $$Q^*(C,N) = 1+\gamma \max_{a'}\lvert Q^*(B,a') \rvert,\,Q^*(C,S) = 1+\gamma \max_{a'}\lvert Q^*(C,a') \rvert$$
> > and
> > $$Q^*(B,N) = -1+\frac\gamma2\lvert \max_{a'}Q^*(A,a')+\max_{a'}Q^*(B,a') \rvert$$
> > $$Q^*(B,S) = -1+\frac\gamma2\lvert \max_{a'}Q^*(B,a')+\max_{a'}Q^*(C,a') \rvert$$
> > and
> > $$Q^*(C,N) = 1+\frac\gamma2\lvert \max_{a'}Q^*(A,a')+\max_{a'}Q^*(B,a') \rvert,\,Q^*(C,S) = 1+\gamma\max_{a'}\lvert Q^*(B,a') \rvert$$

> **Problem:**
>
> > **Subproblem:**
> >
> > Def of Bellman optimality implies that
> > $$V_h^*(s) = \max_a\lvert r(s,a)+\mathbb{E}_{s'\sim P(.|s,a)}V_{h+1} \rvert$$
> > which implies that since
> > $$\hat \pi_h(s) = \mathrm{argmax}_ar(s,a)+\mathrm{\mathbb{E}}_{s'\sim P(s,a)}\lvert V_{h+1}^*(s') \rvert$$
> > if $a=\hat \pi_h(s)$ implies that
> > $$V_h^*(s) = r(s,\hat \pi_h(S)) + \mathrm{\mathbb{E}}_{s'\sim P(s,\hat \pi_h(s))}\lvert V^*_{h+1}(s') \rvert$$
> > which implies the inequality holds 0◻
>
> > **Subproblem:**
> >
> > Asssume for $h$, $P(h)=V_{h}^{\hat\pi}(s)\geq V_{h}^*(s)\forall s$.
> >
> > Thus
> > $$V_h^{\hat\pi}(s)\geq V_{h}^*(s) \Rightarrow \mathrm{\mathbb{E}}_{s'\sim P(\cdot|s,\hat\pi(s))}\lvert V_h^*(s) \rvert\geq \mathrm{\mathbb{E}}_{s'\sim P(\cdot|s,\hat\pi(s))}\lvert V_h^*(s) \rvert$$
> > which implies that
> > $$r(s,\hat\pi_h(s))+ \mathrm{\mathbb{E}}_{s'\sim P(\cdot|s,\hat\pi(s))}\lvert V_h^*(s) \rvert\geq r(s,\hat\pi_h(s)) + \mathrm{\mathbb{E}}_{s'\sim P(\cdot|s,\hat\pi(s))}\lvert V_h^*(s) \rvert = V_{h-1}^*(s)\forall s$$
> > Now given 2a implies
> > $$V_{h-1}^{\hat\pi}(s) = r(s,\hat\pi(s)) + \mathrm{\mathbb{E}}_{s'\sim P(\cdot|s,\hat\pi(s))}\lvert V_h^*(s') \rvert$$
> > And thus $V_{h-1}^{\hat\pi(s)}\geq V_h^*(s)\forall s$
> >
> > And since 2a implies given $h=H$ that
> > $V_h^*(s)\leq r(s,\hat \pi(s)) \forall s$ which implies that by
> > definition since $V_h^{\hat\pi}(s)= r(s,\hat\pi(s))\forall$ implies
> > that $V_h^{\hat\pi}(s)\geq V_h^*(s)\forall s$ given $h=H$. Thus the
> > base case is satisifed ans since we showed the recursive step
> > implies by mathematical induction all cases are satisfied. 0◻
>
> > **Subproblem:**
> >
> > Assume that $V_h^\pi = V_h^*(s)\forall s$. Then
> > $$V_{h-1}^\pi(s) = r(s,\pi_{h-1}(s))+\mathrm{\mathbb{E}}_{s'\sim P(\cdot |s,\pi_{h-1}(s))}\lvert V_h^\pi(s') \rvert$$
> > And $V^\pi$ being bellman optimal implies $\pi$ is as well so
> > $$V_h^*(s)=r(s,\pi_{h-1}(s)) + \mathrm{\mathbb{E}}_{s'\sim P(\cdot|s,\pi_{h-1}(s))}\lvert V_h^*(s') \rvert\forall s \Rightarrow V_{h-1}^\pi(s)=V_{h-1}^*(s)\forall s$$
> >
> > Now since given $h=H$ implies that
> > $V_h^*(s)=r(s,\pi^*(s)) \forall s$. This implies that since
> > $V^\pi(s)$ is bellman optimal implies that
> > $$V_h^*(s)=r(s,\pi_h(s)) + \mathrm{\mathbb{E}}_{s'\sim P(\cdot|s,\pi_h(s))}\lvert V_{h+1}^*(s') \rvert\forall$$
> > So $V_h^\pi(s) = r(s,\pi_h(s))$ forall $s$. Thus we have shown the
> > base case and the recursive step is true which implies by
> > mathematical induction all cases are satisfied 0◻

> **Problem:**
>
> > **Subproblem:**
> >
> > Now because$d_h^{\mu_0}= (P^h_\pi)^T \mu_0$, since $\mu_0$ and
> > $P_{\pi}$ are probability distribtions and transition matricies, it
> > follows that all their entries are nonnegative and sum to $1$. Now
> > since $P_\pi P_\pi$ is a matrix multiplication implies that each
> > entry is the sum of a series of multiplcations of entires in
> > $P_\pi$. Since all entires are nonnegative implies $P_\pi P_\pi$ is
> > nonnegative. This implies that $P_\pi^h = P_\pi P_\pi^{h-1}$ has
> > nonnegative entires. Thus trivially $(P_\pi^h)^T$ has nonnegative
> > entires, which implies since similarly $(P_\pi^h)^T$ and $\mu_0$
> > have nonnegative entires the product will as well. Thus
> > $d_\gamma^{\mu_0}(s)\geq 0 \forall s$. We also know that since
> > $d_h^{\mu_0}$ is a probability distribtion implies that
> > $\sum_{s\in S\forall s}d_h^{\mu_0} = 1$
> >
> > Now since
> > $$\sum_{s\in S \forall s}d_\gamma^{\mu_0} = (1-\gamma)\sum_{h=0}^\infty \gamma_h \sum_{s\in S\forall s}(P_\pi^h)^T \mu_0(s) =$$
> > $$(1-\gamma)\sum_{h=0}^\infty \gamma h \sum_{s\in S\forall s}d_h^{\mu_0}(s) = (1-\gamma) \sum_{h=1}^\infty \gamma_h = (1-\gamma)\frac{1}{1-\gamma} = 1$$
> > Then
>
> > **Subproblem:**
> >
> > Let $C = \frac1{1-\gamma}$.
>
> > **Subproblem:**
> >
> > Now
> > $$R^\pi(s) = \mathrm{\mathbb{E}}_{a\sim \pi(\cdot | s)}\lvert r(s,a) \rvert = \sum_{a'}\pi(a'|s)r(s,a') = \begin{bmatrix}R^\pi(s_1)]\\ \vdots \\ R^\pi(s_n)\end{bmatrix}$$
> >
> > And
> > $$P_\pi(s'|s)=\mathrm{\mathbb{E}}_{a\sim\pi(\cdot|s)}\lvert P(s'|s,a) \rvert=\sum_a \pi(a|s)P(s'|s,a) =$$
> > $$\begin{bmatrix}
> >         P_\pi(s_1|s_1) & P_\pi(s_2|s_1) & \cdots & P_\pi(s_n|s_1) \\ 
> >         P_\pi(s_1|s_2) & P(\pi(s_2|s_2)) & \cdots & P_\pi(s_n|s_2) \\ 
> >         \vdots & \vdots & \ddots & \vdots \\ 
> >         P_\pi(s_1|s_n) & P_\pi(s_2|s_n) & \cdots & P_\pi(s_n|s_n)
> >       \end{bmatrix}$$
> >
> > Which implies that
> > $$V^\pi(s) = \mathrm{\mathbb{E}}_{a\sim\pi(\cdot|s)}\lvert r(s,a)+\gamma\sum_{s'}P(s'|s,a)V^\pi(s') \rvert = \sum_a\pi(a|s)\lvert r(s,a)+\gamma\sum_{s'}P(s'|s,a)V^\pi (s') \rvert$$
> > $$= \sum_a \pi(a|s)r(s,a)+\gamma\sum_{s'}\sum_a\pi(a|s)P(s'|s,a)V^\pi(s') = R^\pi(s)\gamma \sum_{s'}P_\pi (s'|s)V^\pi(s') \Rightarrow$$
> > that $V^\pi = R^\pi + \gamma P_\pi V^\pi$

> **Problem:**
>
> > **Subproblem:**
> >
> > Given $$\lvert Q^{\pi^{h+1}}(s,a)-Q^*(s,a) \rvert =$$
> > $$\lvert r(s,a) + \gamma\mathrm{\mathbb{E}}_{s'\sim P(\cdot  \rverts,a)}Q^{\pi^{h+1}}(s',\pi^{t+1}(s')) - r(s,a) - \gamma\mathrm{\mathbb{E}}_{s\sim P(\cdot | s,a)}\max_{a'}Q^*(s',a')| =$$
> > $$\gamma\mathrm{\mathbb{E}}_{s'\sim P(\cdot |s,a)}\lvert Q^{\pi^{h+1}}(s',\pi^{t+1}(s'))-\max_{a'}Q^*(s',a') \rvert$$
> > $$= \gamma\mathrm{\mathbb{E}}_{s'\sim P(\cdot |s,a)}\lvert Q^{\pi^{h+1}}(s',\pi^{t+1}(s'))-Q^*(s',\pi^*(s')) \rvert=$$
> > $$= \gamma\mathrm{\mathbb{E}}_{s'\sim P(\cdot |s,a)}\lvert Q^{\pi^{h+1}}(s',\pi^{t+1}(s')) -Q^*(s',\pi^{t+1}(s'))+Q^*(s',\pi^{t+1}(s'))-Q^*(s',\pi^*(s')) \rvert\Rightarrow$$
> > $$= \gamma\mathrm{\mathbb{E}}_{s'\sim P(\cdot |s,a)}\lvert Q^{\pi^{h+1}}(s',\pi^{t+1}(s')) -Q^*(s',\pi^{t+1}(s'))+Q^*(s',\pi^{t+1}(s'))-Q^*(s',\pi^*(s')) \rvert$$
> > $$\leq\gamma\mathrm{\mathbb{E}}_{s'\sim P(\cdot |s,a)}\lvert Q^{\pi^{h+1}}(s',\pi^{h+1}(s')) - Q^*(s',\pi^{h+1}) \rvert$$
> > since optimal policy implies that
> > $Q^*(s',\pi^*(s'))\geq Q^*(s',\pi^{t+1}(s')) \Rightarrow Q^*(s',\pi^{t+1}(s'))- Q^*(s',\pi^*(s')) \leq 0$.
> >
> > Now since
> > $$Q^{\pi^{h+1}}(s',\pi^{h+1}(s')) - Q^*(s',\pi^{h+1}) = \gamma\mathrm{\mathbb{E}}_{s''\sim P(\cdot |s', \pi^{h+1}(s'))}\lvert Q^{\pi^{h+1}}(s'', \pi^{h+1}(s''))-Q^*(s', \pi^*(s')) \rvert$$
> > implies a recursion such that
> > $$\lvert Q^{\pi^{h+1}}(s,\pi^{h+1}(s)) - Q^*(s,\pi^{h+1}) \rvert \leq \gamma \lvert \mathrm{\mathbb{E}}_{s'\sim P(\cdot \rverts,a)}Q^{\pi^{h+1}}(s',\pi^{h+1}(s')) - Q^*(s',\pi^{h+1})| \leq$$
> > $$\lvert \gamma^2\mathrm{\mathbb{E}}_{s'\sim P(\cdot \rverts,a)}\mathrm{\mathbb{E}}_{s''\sim P(\cdot|s',\pi^{h+1}(s')} 
> > Q^{\pi^{h+1}}(s'',\pi^{h+1}(s'')) - Q^*(s'',\pi^{h+1})|\leq \cdots \leq \gamma^\infty (\mathrm{\mathbb{E}}\cdots)= \gamma^\infty C = 0$$
> > since $\gamma$ is bounded by $[0,1)$ so
> > $\lim_{n\to\infty}\gamma^n = 0$ and since
> > $0\leq \lvert Q^{\pi^{h+1}}(s,\pi^{h+1}(s)) - Q^*(s,\pi^{h+1}) \rvert$
> > implies that
> > $$Q^{\pi^{h+1}}(s,\pi^{h+1}(s)) = Q^*(s,\pi^{h+1})\forall s,a$$
>
> > **Subproblem:**
> >
> > Monotonic improvment of PI implies that
> > $Q^{\pi^{h+1}}(s,a)\geq Q^{\pi^h}(s,a)\forall s,a$.
> >
> > Now given the inequality, suppose
> > $Q^{\pi^{h+1}}(s,a) = Q^{\pi^h}(s,a)$. Then 4a implies tha t
> > $\pi^{h+1}$ is an optimal policy.
> >
> > Now suppose $Q^{\pi^{h+1}}(s,a) = Q^{\pi^h}(s,a)$. Thus $\forall s$
> > since there exists $|A|$ actions implies the number of times $s$ can
> > be improved by $\pi^h$ is bounded by $A|$. Now since $A$ actions $S$
> > states imply a finite bound of $|A|^|S|$ policies given that the
> > number of total interations is finite implies that there exists a
> > finite point where any more iterations contiutally yields
> > $Q^{\pi^{h+1}}(s,a)=Q\pi^h(s,a)$ which implies that $\pi^h$ is
> > optimal 0◻
