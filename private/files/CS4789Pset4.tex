\documentclass[12pt]{amsart}
\usepackage{austin}

\title{CS4789}

\begin{document}
  \maketitle
  \begin{problem}
    \begin{subproblem}
      Since we have fixed $A=A^{\pi_{\theta_t}}(s,a)$, we know that
      \[r(\theta)=\frac{\pi_\theta(a|s)}{\pi_{\theta_t}(a|s)}\]
      Thus each term is 
      \[r(\theta)A, (1-\epsilon)A, (1+\epsilon)A\]
      If the first term is the min, then the gradient becomes
      \begin{align*}
        \nabla_\theta[r(\theta)]A&=A\nabla_\theta[\frac{\pi_\theta(a|s)}{\pi_{\theta_t}(a|s)}] \\
                                 &= A\,\nabla_\theta r(\theta) \\
                                 &= A\,\nabla_\theta\!\Bigl[\frac{\pi_\theta(a\mid s)}{\pi_{\theta_t}(a\mid s)}\Bigr] \\
                                 &= A\,\frac{\pi_\theta(a\mid s)}{\pi_{\theta_t}(a\mid s)}\,\nabla_\theta\log\pi_\theta(a\mid s) \\
                                 &= A\,r(\theta)\,\nabla_\theta\log\pi_\theta(a\mid s).
      \end{align*}
      For the second term, it becomes
      \begin{align*}
        \nabla_\theta[(1-\epsilon)A] &= 0
      \end{align*}
      And for the third term, it becomes
      \begin{align*}
        \nabla_\theta[(1+\epsilon)A] &= 0
      \end{align*}
      Since they are constant with respect to theta.
    \end{subproblem}
    \begin{subproblem}
      When $r(\theta)$ drifts above $1+\epsilon$ with $A>0$ or below $1-\epsilon$ with $A<0$, we choose the clipped bound instead of the main function. This causes the gradient to get set to 0, This implies that $r(\theta)$ can't drift above or below these bounds. This means that the polciy ratio cannot increase more than its bounded $[1-\epsilon, 1+\epsilon]$ keeping policy updates close to $\pi_{\theta_t}$.
    \end{subproblem}
    \begin{subproblem}
      Suppose instead we use
      \[\hat l_{\text{final}}(\theta)=\sum_{s,a}\text{clip}\ab(\frac{\pi_\theta(a|s)}{\pi_{\theta_t}}(a|s), 1-\epsilon, 1+\epsilon)\cdot A^{\pi_{\theta_t}}(s,a)\]
      Instead of having a minimization. 

      Then if $A>0$ and $r<1-\epsilon$ the function without the min would always be clipped by $(1-\epsilon)A$ for any state action pair which has gradient $0$. This implies that if $r$ falls below the $1-\epsilon$ lower bound, there is no ability for the gradient to be in a direction to increase it.

      This contradicts the function with the min because if it falls below the $1-\epsilon$ bound the gradient pushes $r(\theta)$ up so $min(rA, (1-\epsilon)A)=rA$. This then creates the gradient $A\,r(\theta)\,\nabla_\theta\log\pi_\theta(a\mid s)$ from the previous part, allowing a corrective gradient to bring it back up.

      Similarly, if $A<0$ and $r>1+\epsilon$ the function without the min would always be clipped by $(1+\epsilon)A$ for any state action pair with gradient $0$. This implies that if $r$ falls above the $1+\epsilon$ upper bound, there is no ability for the gradient to be in a direction to decrease it.

      This similarly contradicts the function with the min because if it rises above the $1+\epsilon$ bound the gradient pushes $r(\theta)$ down so $min(rA, (1+\epsilon)A)=rA$. This then creates the gradient $A\,r(\theta)\,\nabla_\theta\log\pi_\theta(a\mid s)$ from the previous part, allowing a corrective gradient to bring it back down.
    \end{subproblem}
  \end{problem}
  \begin{problem}
    \begin{subproblem}
      Set $\pi_\text{ref}=\V{\frac13&\frac13&\frac13}$. Then $\frac13\leq \frac13\leq\frac13$. Now define $\pi_0 = \V{0.3&0.2&0.5}$. Then $0.2\leq0.3\leq0.5$. Thus we get that 
      \begin{align*}
        \lat_{DPO}(\pi_\theta;\pi_{\text{ref}}) &= -\expect_{x,y_w,y_l\in \mathcal{D}}\ab[\log\sigma(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)}-\beta\log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)})] \\
        \intertext{Simplifies to }
                                                &= -\expect_{x,y_w,y_l\in \mathcal{D}}\ab[\log\sigma(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_\theta(y_l|x)})] \\
                                                \intertext{Since $\pi_\text{ref}(y|x)$ is always equal to $\frac13$. Thus if we sample with single prefrence ordering $y_1\leq y_2$ we get when going from $\pi_0$ to the next iteration}
                                                &= -\expect_{x,y_2,y_1\in \mathcal{D}}\ab[\log\sigma(\beta\log\frac{\pi_\theta(y_2|x)}{\pi_\theta(y_1|x)})] \\
                                                &= -\expect_{x,y_2,y_1\in \mathcal{D}}\ab[\log\sigma(\beta\log\frac{3}{2})] \\
      \end{align*}
      Thus if we create polices $\pi_1, \pi_2$ that have a good loss function, we must maximize the ratio of $\frac{\pi_\theta(y_2|x)}{\pi_\theta(y_1|x)}$. This also implies that polices with equivalent ratios have the same loss function since we are only sampling once. Thus after update the some possible answers are arbitrarly setting $\pi_1(y_3)=\frac12$. Then say
      \[\frac{\pi_1(y_2)}{\pi_1(y_2)}=1.75=r\]
      Thus since probability spaces must stay normalized we are constrained to  
      \[\pi_1(y_1)+1.75\pi_1(y_1)=\frac12\implies 2.75\pi_1(y_1)=\frac12\implies\pi_1(y_1)=\frac2{11}\approx0.18, \pi_1(y_2)=\frac{22}{69}\approx0.31\]
      We could also set $\pi_2(y_3)=\frac14$. Then since the ratio must be 1.5 implies that 
      \[\pi_2(y_1)+1.75\pi_2(y_1)=\frac34\implies  2.75\pi_2(y_1)=\frac34\implies\pi_2(y_1)=\frac{3}{11}\approx0.27, \pi_2(y_2)=\frac{21}{44}\approx0.47\]
      Thus $\pi_1=\V{\frac2{11}&\frac{22}{69}&0.5}, \pi_2=\V{\frac{3}{11}&\frac{21}{44}&0.25}$ wich equivlanet loss functions, but $\pi_1$ satisifes $y_1\leq y_2\leq y_3$ with $\pi_2$ does not.
      Thus our loss functions become 
      \[\lat_{DPO}=\log(\frac1{1+e^{-\beta r}})\]
      With $r$ being the ratio between elements. Thus the loss with $\beta=1$ is
      \begin{align*}
        \lat(\pi)_{\text{ref}}&=\log(\frac1{1+e^{-\beta 1}})\approx -0.31\\
        \lat(\pi_0)&=\log(\frac1{1+e^{-\beta 1.5}})\approx -0.2\\
        \lat(\pi_1)&=\log(\frac1{1+e^{-\beta 1.75}})\approx -0.16\\
        \lat(\pi_2)&=\log(\frac1{1+e^{-\beta 1.75}})\approx-0.16
      \end{align*}
      This means that DPO has a blindness between unobserved comparisons regarding polices.
    \end{subproblem}
    \begin{subproblem}
      %Let $\epsilon\in(0,$. Then 
      For $\pi_{\text{ref}}(\cdot|x), \pi_{\theta}(\cdot|x)$ let 
      \[\begin{array}{|c|c|}
        \hline
        \pi_{\text{ref}}(y_1|x) & \frac12\\
        \pi_{\text{ref}}(y_2|x) & \frac12 \\
        \pi_{\text{ref}}(y_3|x) & 0\\
        \pi_{\theta}(y_1|x) & 0\\
        \pi_{\theta}(y_2|x) & \frac12\\
        \pi_{\theta}(y_3|x) & \frac12\\
        \hline
      \end{array}\]
      With $\pi_{\text{ref}}(y_1|x)=\frac12\leq\pi_{\text{ref}}(y_2|x)=\frac12$ and $\pi_{\theta}(y_1|x) = 0\leq \pi_{\theta}(y_2|x) = \frac12$
      Then we find that 
      \begin{align*}
        \lat_{DPO}(\pi_\theta;\pi_{\text{ref}}) &= -\expect_{x,y_w,y_l\in \mathcal{D}}\ab[\log\sigma(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)}-\beta\log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)})] \\
                                \intertext{Now since we are only sampling $(y_w,y_l)=(y_2,y_1)$}
                                &= -\expect_{x,y_w,y_l\in \mathcal{D}}\ab[\log\sigma(\beta\log\frac{\pi_\theta(y_w|x)\pi_{\text{ref}}(y_l|x)}{\pi_{\text{ref}}(y_w|x)\pi_\theta(y_l|x)})] \\
                                \intertext{With notably $\sigma(z)=\frac1{1+e^{-z}}$}
                                &= -\expect_{x,y_2,y_1\in \mathcal{D}}\ab[\log\sigma(\beta\log\frac{\pi_\theta(y_2|x)\pi_{\text{ref}}(y_1|x)}{\pi_{\text{ref}}(y_2|x)\pi_\theta(y_1|x)})] \\
                                \intertext{Thus plugging in the values we get}
                                &= -\expect_{x,y_2,y_1\in \mathcal{D}}\ab[\log\sigma(\beta\log\frac{\pi_\theta(y_2|x)}{\pi_\theta(y_1|x)})] \\
                                &= -\expect_{x,y_2,y_1\in \mathcal{D}}\ab[\log\sigma(\beta\log\ab(\frac{\frac12}{0}))] \\
                                \intertext{With asymotic behavior $\beta\log(\frac{\frac12}{0})=+\infty$ thus}
                                &= -\expect_{x,y_2,y_1\in \mathcal{D}}\ab[\log\frac1{1+e^{-(+\infty)}}] \\
                                &= -\expect_{x,y_2,y_1\in \mathcal{D}}\ab[\log(1)] \\
                                &= 0
      \end{align*}
      And for KL Difergence we get
      \begin{align*}
        D_{KL}(\pi_\theta||\pi_{\text{ref}}) &= \sum_{i=1}^3\theta(y_i|x)\log\frac{\pi_\theta(y_i|x)}{\pi_{\text{ref}}(y_i|x)} \\
                                      &= 0 + \frac12\log\frac{\frac12}{\frac12}+\frac12\log\frac{\frac12}{0}\\
                                      &= +\infty
      \end{align*}
      Thus we shown that we can pick a policy for $\lat_{DPO}=0$ and $KL(\pi||\pi_\text{ref})=+\infty$
    %
    %  \[\begin{array}{c|c}
    %    \pi_{\text{ref}}(y_1|x) & \epsilon\\
    %    \pi_{\text{ref}}(y_2|x) & \epsilon \\
    %    \pi_{\text{ref}}(y_3|x) & 1-2\epsilon\\
    %    \pi_{\theta}(y_1|x) & \frac12\\
    %    \pi_{\theta}(y_2|x) & \frac12\\
    %    \pi_{\theta}(y_3|x) & 0\\
    %  \end{array}\]
    %  Then we find that
    %  \begin{align*}
    %  \lat_{DPO}(\pi_\theta;\pi_{\text{ref}}) &= -\expect_{x,y_w,y_l\in \mathcal{D}}\ab[\log\sigma(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)}-\beta\log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)})] \\
    %                                          &= -\expect_{x,y_w,y_l\in \mathcal{D}}\ab[\log\sigma(\beta\log\frac{\pi_\theta(y_w|x)\pi_{\text{ref}}(y_l|x))}{\pi_{\text{ref}}(y_w|x)\pi_\theta(y_l|x)}] \\
    %                                          &= -\expect_{x,y_1,y_2\in \mathcal{D}}\ab[\log\sigma(\beta\log\frac{\pi_\theta(y_1|x)\pi_{\text{ref}}(y_2|x))}{\pi_{\text{ref}}(y_1|x)\pi_\theta(y_2|x)}] \\
    %                                          &= -\expect_{x,y_1,y_2\in \mathcal{D}}\ab[\log\sigma(\beta\log(1)] \\
    %                                          &= 0\
    %  \end{align*}
    %While 
    %\begin{align*}
    %  D_{KL}(\pi||\pi_{\text{ref}}) &= \sigma_y \pi_\theta(y|x)\log\frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)} \\
    %                                &= 2\ab(\frac12\log{\frac{1}{2\epsilon}})+0 \\
    %                                &= \log(\frac1{2\epsilon})
    %\end{align*}
    %Which is unbounded
    \end{subproblem}
    
  \end{problem}
   
  
\end{document}
